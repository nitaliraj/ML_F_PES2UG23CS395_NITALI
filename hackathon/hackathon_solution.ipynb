{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3130aa",
   "metadata": {
    "id": "ca3130aa"
   },
   "source": [
    "# ML Hackathon: HMM and Reinforcement Learning Word Completion\n",
    "\n",
    "## Problem Statement\n",
    "This notebook implements:\n",
    "1. **Part 1**: Hidden Markov Model for letter probability estimation\n",
    "2. **Part 2**: Reinforcement Learning agent using HMM guidance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed36fd",
   "metadata": {
    "id": "5eed36fd"
   },
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ad6d155",
   "metadata": {
    "id": "1ad6d155"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "import math, re, random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e732e3",
   "metadata": {
    "id": "c8e732e3"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "612a98e1",
   "metadata": {
    "id": "612a98e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 50000 words\n",
      "Test size: 2000 words\n"
     ]
    }
   ],
   "source": [
    "# Load corpus\n",
    "with open('Data/Data/corpus.txt', 'r') as f:\n",
    "    corpus_words = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "# Load test data\n",
    "with open('Data/Data/test.txt', 'r') as f:\n",
    "    test_words = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Corpus size: {len(corpus_words)} words\")\n",
    "print(f\"Test size: {len(test_words)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb1d6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_proper_train_test_split(all_words, test_size=0.1):\n",
    "    \"\"\"Split corpus into train/test ensuring clean data.\"\"\"\n",
    "    all_words = list(set(w.lower().strip() for w in all_words if w.strip()))\n",
    "    random.shuffle(all_words)\n",
    "    split_idx = int(len(all_words) * (1 - test_size))\n",
    "    return all_words[:split_idx], all_words[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f88a7d",
   "metadata": {
    "id": "61f88a7d"
   },
   "source": [
    "---\n",
    "# Part 1: Hidden Markov Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef954618",
   "metadata": {
    "id": "ef954618"
   },
   "outputs": [],
   "source": [
    "class ContextualHiddenMarkovModel:\n",
    "    def __init__(self, order=2, smoothing=0.01):\n",
    "        \"\"\"HMM for character-level probability estimation - keep it simple.\"\"\"\n",
    "        self.order = order\n",
    "        self.smoothing = smoothing\n",
    "        self.transition_counts = defaultdict(Counter)\n",
    "        self.positional_counts = defaultdict(Counter)\n",
    "        self.letter_priors = Counter()\n",
    "        self.vocabulary = set()\n",
    "        self.corpus_words = []\n",
    "        self.start_token = '<S>'\n",
    "        self.end_token = '<E>'\n",
    "\n",
    "    def train(self, words: List[str]):\n",
    "        print(\"Training HMM...\")\n",
    "        self.corpus_words = list(set(w.lower().strip() for w in words if w.strip()))\n",
    "\n",
    "        for word in tqdm(self.corpus_words, desc=\"Building counts\"):\n",
    "            padded = self.start_token * (self.order - 1) + word + self.end_token\n",
    "            self.vocabulary.update(c for c in word if c.isalpha())\n",
    "\n",
    "            for i in range(len(padded) - self.order + 1):\n",
    "                context = padded[i:i+self.order-1]\n",
    "                nxt = padded[i+self.order-1]\n",
    "                pos = min(i, 15)\n",
    "                self.transition_counts[context][nxt] += 1\n",
    "                self.positional_counts[(pos, context[-1])][nxt] += 1\n",
    "                self.letter_priors[nxt] += 1\n",
    "\n",
    "        self.vocab_list = sorted(self.vocabulary)\n",
    "        print(f\"‚úÖ Vocabulary size: {len(self.vocab_list)}\")\n",
    "\n",
    "    def _prob(self, counter: Counter, char: str) -> float:\n",
    "        total = sum(counter.values())\n",
    "        V = len(self.vocabulary)\n",
    "        return (counter[char] + self.smoothing) / (total + self.smoothing * V)\n",
    "\n",
    "    def get_conditional_prob(self, context: str, pos: int, char: str) -> float:\n",
    "        \"\"\"Get probability of character given context and position.\"\"\"\n",
    "        p1 = self._prob(self.transition_counts[context], char)\n",
    "        p2 = self._prob(self.positional_counts[(pos, context[-1])], char)\n",
    "        p3 = (self.letter_priors[char] + self.smoothing) / \\\n",
    "             (sum(self.letter_priors.values()) + self.smoothing * len(self.vocabulary))\n",
    "        \n",
    "        # Standard weighting\n",
    "        return 0.70*p1 + 0.20*p2 + 0.10*p3\n",
    "\n",
    "    def _get_context_at_position(self, word_list: List[str], pos: int) -> str:\n",
    "        \"\"\"Extract context for position.\"\"\"\n",
    "        context_start = max(0, pos - (self.order - 1))\n",
    "        context = word_list[context_start:pos]\n",
    "        needed_padding = (self.order - 1) - len(context)\n",
    "        if needed_padding > 0:\n",
    "            context = [self.start_token] * needed_padding + context\n",
    "        context = ['a' if c == '_' else c for c in context]\n",
    "        return ''.join(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f0a41",
   "metadata": {
    "id": "722f0a41"
   },
   "source": [
    "## Train HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "462ddceb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "462ddceb",
    "outputId": "080410f4-cd4d-46c1-ce6b-d55af931efd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HMM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building counts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49398/49398 [00:03<00:00, 12518.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vocabulary size: 26\n",
      "‚úÖ HMM trained on 50000 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train HMM on full corpus - simple and reliable\n",
    "hmm_model = ContextualHiddenMarkovModel(order=2, smoothing=0.01)\n",
    "hmm_model.train(corpus_words)\n",
    "\n",
    "print(f\"‚úÖ HMM trained on {len(corpus_words)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c0f1c",
   "metadata": {
    "id": "922c0f1c"
   },
   "source": [
    "---\n",
    "# Part 2: Reinforcement Learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a8d2050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedLetterGuessingEnv:\n",
    "    \"\"\"Compact Hangman environment.\"\"\"\n",
    "    def __init__(self, vocabulary: set, max_wrong_guesses: int = 6):\n",
    "        self.alphabet = sorted(list(vocabulary))\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(self.alphabet)}\n",
    "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
    "        self.max_wrong_guesses = max_wrong_guesses\n",
    "        \n",
    "    def reset(self, target_word: str):\n",
    "        self.target_word = target_word.lower().strip()\n",
    "        self.guessed_letters = set()\n",
    "        self.wrong_guesses = 0\n",
    "        self.repeated_guesses = 0\n",
    "        self.done = False\n",
    "        self.current_masked = ['_'] * len(self.target_word)\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Compact state: (word_length, num_blanks, lives_left, last_char).\"\"\"\n",
    "        masked_str = ''.join(self.current_masked)\n",
    "        num_blanks = masked_str.count('_')\n",
    "        lives_left = self.max_wrong_guesses - self.wrong_guesses\n",
    "        word_length = len(self.target_word)\n",
    "        last_char = ''\n",
    "        for c in reversed(self.current_masked):\n",
    "            if c != '_':\n",
    "                last_char = c\n",
    "                break\n",
    "        return (word_length, num_blanks, lives_left, last_char)\n",
    "    \n",
    "    def get_masked_word(self):\n",
    "        return ''.join(self.current_masked)\n",
    "    \n",
    "    def step(self, action: int):\n",
    "        if self.done:\n",
    "            return self._get_state(), 0, True, {}\n",
    "        \n",
    "        if not isinstance(action, (int, np.integer)) or action < 0 or action >= len(self.alphabet):\n",
    "            return self._get_state(), -5.0, False, {'error': True}\n",
    "        \n",
    "        guessed_char = self.idx_to_char[action]\n",
    "        \n",
    "        if guessed_char in self.guessed_letters:\n",
    "            self.repeated_guesses += 1\n",
    "            reward = -3.0\n",
    "            info = {'repeated': True, 'correct': False}\n",
    "        else:\n",
    "            self.guessed_letters.add(guessed_char)\n",
    "            \n",
    "            if guessed_char in self.target_word:\n",
    "                count = sum(1 for i, c in enumerate(self.target_word) if c == guessed_char)\n",
    "                for i, c in enumerate(self.target_word):\n",
    "                    if c == guessed_char:\n",
    "                        self.current_masked[i] = guessed_char\n",
    "                reward = 2.0 * count\n",
    "                info = {'repeated': False, 'correct': True, 'count': count}\n",
    "            else:\n",
    "                self.wrong_guesses += 1\n",
    "                reward = -1.0\n",
    "                info = {'repeated': False, 'correct': False}\n",
    "        \n",
    "        if '_' not in self.current_masked:\n",
    "            self.done = True\n",
    "            reward += 10.0\n",
    "            info['won'] = True\n",
    "        elif self.wrong_guesses >= self.max_wrong_guesses:\n",
    "            self.done = True\n",
    "            reward -= 5.0\n",
    "            info['won'] = False\n",
    "        \n",
    "        return self._get_state(), reward, self.done, info\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            'wrong_guesses': self.wrong_guesses,\n",
    "            'repeated_guesses': self.repeated_guesses,\n",
    "            'won': '_' not in self.current_masked and self.wrong_guesses < self.max_wrong_guesses\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da6f683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedQLearningAgent:\n",
    "    \"\"\"Q-Learning with HMM-Q blending - simple and stable.\"\"\"\n",
    "    def __init__(self, n_actions: int, learning_rate: float = 0.1,\n",
    "                 discount_factor: float = 0.9, epsilon: float = 0.2,\n",
    "                 hmm_blend_weight: float = 0.97):\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.998\n",
    "        self.hmm_blend_weight = hmm_blend_weight\n",
    "        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n",
    "        self.idx_to_char = {}\n",
    "        self.visit_counts = defaultdict(int)\n",
    "\n",
    "    def get_action(self, state, action_probs=None, guessed_letters=set()):\n",
    "        if state is None:\n",
    "            return 0\n",
    "\n",
    "        valid_mask = np.ones(self.n_actions)\n",
    "        for letter in guessed_letters:\n",
    "            for idx, char in self.idx_to_char.items():\n",
    "                if char == letter:\n",
    "                    valid_mask[idx] = 0\n",
    "                    break\n",
    "        \n",
    "        if np.sum(valid_mask) == 0:\n",
    "            valid_mask = np.ones(self.n_actions)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore with HMM guidance\n",
    "            if action_probs is not None and len(action_probs) == self.n_actions:\n",
    "                masked_probs = action_probs * valid_mask\n",
    "                if masked_probs.sum() > 0:\n",
    "                    masked_probs = masked_probs / masked_probs.sum()\n",
    "                    return np.random.choice(self.n_actions, p=masked_probs)\n",
    "            valid_actions = np.where(valid_mask > 0)[0]\n",
    "            return np.random.choice(valid_actions) if len(valid_actions) > 0 else 0\n",
    "        else:\n",
    "            # Exploit: Blend Q-values with HMM\n",
    "            state_key = str(state)\n",
    "            q_values = self.q_table[state_key].copy()\n",
    "            \n",
    "            if action_probs is not None and len(action_probs) == self.n_actions:\n",
    "                hmm_scores = np.log(action_probs + 1e-10)\n",
    "                combined_scores = (1 - self.hmm_blend_weight) * q_values + \\\n",
    "                                  self.hmm_blend_weight * hmm_scores\n",
    "            else:\n",
    "                combined_scores = q_values\n",
    "            \n",
    "            combined_scores[valid_mask == 0] = -np.inf\n",
    "            \n",
    "            if np.all(combined_scores == -np.inf):\n",
    "                valid_actions = np.where(valid_mask > 0)[0]\n",
    "                return np.random.choice(valid_actions) if len(valid_actions) > 0 else 0\n",
    "            \n",
    "            return np.argmax(combined_scores)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        if state is None or action < 0 or action >= self.n_actions:\n",
    "            return\n",
    "        state_key = str(state)\n",
    "        next_state_key = str(next_state) if next_state is not None else None\n",
    "        self.visit_counts[state_key] += 1\n",
    "        current_q = self.q_table[state_key][action]\n",
    "        max_next_q = np.max(self.q_table[next_state_key]) if next_state_key else 0\n",
    "        new_q = current_q + self.lr * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[state_key][action] = new_q\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def get_diagnostics(self):\n",
    "        return {\n",
    "            'epsilon': self.epsilon,\n",
    "            'q_table_size': len(self.q_table),\n",
    "            'unique_states': len(self.visit_counts)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ad111f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedHybridAgent:\n",
    "    \"\"\"Hybrid HMM+RL agent with frequency-aware strategy.\"\"\"\n",
    "    def __init__(self, hmm_model: ContextualHiddenMarkovModel, vocabulary: set,\n",
    "                 learning_rate: float = 0.2, epsilon: float = 0.5, \n",
    "                 hmm_blend_weight: float = 0.75, max_wrong: int = 6):\n",
    "        self.hmm = hmm_model\n",
    "        self.env = ImprovedLetterGuessingEnv(vocabulary, max_wrong_guesses=max_wrong)\n",
    "        self.agent = ImprovedQLearningAgent(\n",
    "            n_actions=len(vocabulary),\n",
    "            learning_rate=learning_rate,\n",
    "            epsilon=epsilon,\n",
    "            hmm_blend_weight=hmm_blend_weight\n",
    "        )\n",
    "        self.char_to_idx = self.env.char_to_idx\n",
    "        self.idx_to_char = self.env.idx_to_char\n",
    "        self.agent.idx_to_char = self.idx_to_char\n",
    "        \n",
    "        # Common English letter frequencies (helps early in game)\n",
    "        self.common_letters = 'etaoinshrdlcumwfgypbvkjxqz'\n",
    "        \n",
    "        print(f\"‚úÖ Agent: {len(vocabulary)} actions, HMM blend={hmm_blend_weight}\")\n",
    "\n",
    "    def get_letter_probabilities_from_hmm(self, masked_word: str, guessed_letters: set) -> np.ndarray:\n",
    "        probs = np.zeros(len(self.env.alphabet))\n",
    "        blank_positions = [i for i, c in enumerate(masked_word) if c == '_']\n",
    "        \n",
    "        if len(blank_positions) == 0:\n",
    "            return np.ones(len(self.env.alphabet)) / len(self.env.alphabet)\n",
    "        \n",
    "        # Calculate HMM probabilities\n",
    "        for pos in blank_positions:\n",
    "            context = self.hmm._get_context_at_position(list(masked_word), pos)\n",
    "            for char, idx in self.char_to_idx.items():\n",
    "                prob = self.hmm.get_conditional_prob(context, pos, char)\n",
    "                probs[idx] += prob\n",
    "        \n",
    "        probs = probs / len(blank_positions)\n",
    "        \n",
    "        # Boost common letters early in the game - FINE-TUNED\n",
    "        if len(guessed_letters) < 6:  # Extended to 6 guesses\n",
    "            for char in self.common_letters[:10]:  # Top 10 letters (e,t,a,o,i,n,s,h,r,d)\n",
    "                if char in self.char_to_idx:\n",
    "                    idx = self.char_to_idx[char]\n",
    "                    probs[idx] *= 1.5  # 50% boost (increased from 30%)\n",
    "        \n",
    "        return probs / probs.sum() if probs.sum() > 0 else np.ones(len(probs)) / len(probs)\n",
    "\n",
    "    def train(self, training_words: List[str], episodes: int = 20000, eval_interval: int = 2000):\n",
    "        print(f\"Training RL agent for {episodes} episodes...\")\n",
    "        total_rewards = []\n",
    "        win_rate_history = []\n",
    "        \n",
    "        for episode in tqdm(range(episodes)):\n",
    "            target_word = training_words[np.random.randint(len(training_words))]\n",
    "            state = self.env.reset(target_word)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            step_count = 0\n",
    "            max_steps = len(target_word) * 3\n",
    "            \n",
    "            while not done and step_count < max_steps:\n",
    "                masked_word = self.env.get_masked_word()\n",
    "                guessed_letters = self.env.guessed_letters\n",
    "                hmm_probs = self.get_letter_probabilities_from_hmm(masked_word, guessed_letters)\n",
    "                action = self.agent.get_action(state, hmm_probs, guessed_letters)\n",
    "                \n",
    "                if action < 0 or action >= len(self.idx_to_char):\n",
    "                    valid_actions = [i for i in range(len(self.idx_to_char)) \n",
    "                                   if self.idx_to_char[i] not in guessed_letters]\n",
    "                    action = np.random.choice(valid_actions) if valid_actions else 0\n",
    "                \n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                self.agent.update(state, action, reward, next_state)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "            \n",
    "            total_rewards.append(episode_reward)\n",
    "            self.agent.decay_epsilon()\n",
    "            \n",
    "            if episode >= 99:\n",
    "                recent_wins = sum(1 for r in total_rewards[episode-99:episode+1] if r > 5)\n",
    "                win_rate_history.append(recent_wins / 100)\n",
    "            \n",
    "            if episode % eval_interval == 0 and episode > 0:\n",
    "                diag = self.agent.get_diagnostics()\n",
    "                print(f\"\\nEp {episode}: Œµ={diag['epsilon']:.3f}, \"\n",
    "                      f\"States={diag['unique_states']}, \"\n",
    "                      f\"AvgRew={np.mean(total_rewards[-100:]):.2f}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training complete!\")\n",
    "        if win_rate_history:\n",
    "            print(f\"Final win rate: {win_rate_history[-1]*100:.2f}%\")\n",
    "        \n",
    "        return total_rewards, win_rate_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7225b74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7225b74",
    "outputId": "21fbd3e0-16b2-4a4a-db8a-cf5f2b333e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING RL AGENT WITH FREQUENCY BOOST\n",
      "============================================================\n",
      "‚úÖ Agent: 26 actions, HMM blend=0.98\n",
      "Training on 50000 words...\n",
      "Training RL agent for 70000 episodes...\n",
      "Training on 50000 words...\n",
      "Training RL agent for 70000 episodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 5056/70000 [00:23<04:03, 266.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 5000: Œµ=0.010, States=5856, AvgRew=10.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 10090/70000 [00:45<03:25, 291.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 10000: Œµ=0.010, States=6890, AvgRew=6.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà‚ñè       | 15034/70000 [01:02<03:14, 282.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 15000: Œµ=0.010, States=7417, AvgRew=10.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñä       | 20048/70000 [01:23<02:49, 295.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 20000: Œµ=0.010, States=7831, AvgRew=6.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 25060/70000 [01:41<02:34, 291.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 25000: Œµ=0.010, States=8137, AvgRew=6.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 30062/70000 [02:01<02:10, 306.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 30000: Œµ=0.010, States=8385, AvgRew=9.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 35029/70000 [02:22<02:35, 224.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 35000: Œµ=0.010, States=8596, AvgRew=7.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 40084/70000 [02:39<01:42, 291.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 40000: Œµ=0.010, States=8758, AvgRew=6.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 45051/70000 [03:01<01:28, 282.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 45000: Œµ=0.010, States=8970, AvgRew=6.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 50087/70000 [03:18<01:05, 302.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 50000: Œµ=0.010, States=9187, AvgRew=8.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 55062/70000 [03:38<00:46, 321.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 55000: Œµ=0.010, States=9315, AvgRew=8.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 60043/70000 [03:56<00:32, 303.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 60000: Œµ=0.010, States=9429, AvgRew=6.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 65036/70000 [04:17<00:16, 300.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ep 65000: Œµ=0.010, States=9504, AvgRew=6.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70000/70000 [04:34<00:00, 255.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training complete!\n",
      "Final win rate: 44.00%\n",
      "\n",
      "‚úÖ RL agent training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train RL agent - focus on frequency-aware strategy\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING RL AGENT WITH FREQUENCY BOOST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "improved_rl_agent = ImprovedHybridAgent(\n",
    "    hmm_model,\n",
    "    hmm_model.vocabulary,\n",
    "    learning_rate=0.1,\n",
    "    epsilon=0.15,            # Even lower exploration (reduced from 0.2)\n",
    "    hmm_blend_weight=0.98,   # Maximum HMM trust (98%, increased from 97%)\n",
    "    max_wrong=6\n",
    ")\n",
    "\n",
    "# Use focused training sample\n",
    "training_sample = random.sample(corpus_words, min(60000, len(corpus_words)))\n",
    "print(f\"Training on {len(training_sample)} words...\")\n",
    "\n",
    "improved_rewards, improved_win_rates = improved_rl_agent.train(\n",
    "    training_sample,\n",
    "    episodes=70000,  # Increased training (60k ‚Üí 70k)\n",
    "    eval_interval=5000\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ RL agent training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d1139e",
   "metadata": {},
   "source": [
    "## Hackathon Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "942eebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HACKATHON EVALUATION: 2000 games\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Playing Hangman: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:12<00:00, 163.30it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HACKATHON FINAL RESULTS\n",
      "======================================================================\n",
      "Games Played:           2000\n",
      "Games Won:              539 (26.95%)\n",
      "Total Wrong Guesses:    10807\n",
      "Total Repeated Guesses: 0\n",
      "======================================================================\n",
      "üèÜ FINAL SCORE:          -135.00\n",
      "======================================================================\n",
      "\n",
      "Breakdown:\n",
      "  + Success bonus:        53900.00\n",
      "  - Wrong guess penalty:  54035\n",
      "  - Repeat guess penalty: 0\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Results saved to 'hackathon_hybrid_results.csv'\n"
     ]
    }
   ],
   "source": [
    "def evaluate_hackathon_hybrid_agent(improved_agent, test_words: List[str], num_games: int = 2000):\n",
    "    \"\"\"Official hackathon evaluation.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"HACKATHON EVALUATION: {num_games} games\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    eval_words = random.sample(test_words, min(num_games, len(test_words)))\n",
    "    num_games = len(eval_words)\n",
    "    \n",
    "    wins = 0\n",
    "    total_wrong_guesses = 0\n",
    "    total_repeated_guesses = 0\n",
    "    predictions = []\n",
    "    \n",
    "    old_epsilon = improved_agent.agent.epsilon\n",
    "    improved_agent.agent.epsilon = 0  # Greedy\n",
    "    \n",
    "    for word in tqdm(eval_words, desc=\"Playing Hangman\"):\n",
    "        state = improved_agent.env.reset(word)\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        max_steps = len(word) * 3\n",
    "        \n",
    "        while not done and step_count < max_steps:\n",
    "            masked_word = improved_agent.env.get_masked_word()\n",
    "            guessed_letters = improved_agent.env.guessed_letters\n",
    "            hmm_probs = improved_agent.get_letter_probabilities_from_hmm(masked_word, guessed_letters)\n",
    "            action = improved_agent.agent.get_action(state, hmm_probs, guessed_letters)\n",
    "            \n",
    "            if action < 0 or action >= len(improved_agent.idx_to_char):\n",
    "                valid_actions = [i for i in range(len(improved_agent.idx_to_char)) \n",
    "                               if improved_agent.idx_to_char[i] not in guessed_letters]\n",
    "                action = np.random.choice(valid_actions) if valid_actions else 0\n",
    "            \n",
    "            next_state, reward, done, info = improved_agent.env.step(action)\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "        \n",
    "        stats = improved_agent.env.get_stats()\n",
    "        final_guess = improved_agent.env.get_masked_word()\n",
    "        \n",
    "        if stats['won']:\n",
    "            wins += 1\n",
    "        \n",
    "        total_wrong_guesses += stats['wrong_guesses']\n",
    "        total_repeated_guesses += stats['repeated_guesses']\n",
    "        \n",
    "        predictions.append({\n",
    "            'word': word,\n",
    "            'final_guess': final_guess,\n",
    "            'won': stats['won'],\n",
    "            'wrong_guesses': stats['wrong_guesses'],\n",
    "            'repeated_guesses': stats['repeated_guesses']\n",
    "        })\n",
    "    \n",
    "    improved_agent.agent.epsilon = old_epsilon\n",
    "    \n",
    "    success_rate = (wins / num_games) * 100\n",
    "    \n",
    "    # Original scoring formula (reverted)\n",
    "    final_score = (success_rate * 2000) - (total_wrong_guesses * 5) - (total_repeated_guesses * 2)\n",
    "    \n",
    "    results = {\n",
    "        'num_games': num_games,\n",
    "        'wins': wins,\n",
    "        'success_rate': success_rate,\n",
    "        'total_wrong_guesses': total_wrong_guesses,\n",
    "        'total_repeated_guesses': total_repeated_guesses,\n",
    "        'final_score': final_score,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"HACKATHON FINAL RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Games Played:           {num_games}\")\n",
    "    print(f\"Games Won:              {wins} ({success_rate:.2f}%)\")\n",
    "    print(f\"Total Wrong Guesses:    {total_wrong_guesses}\")\n",
    "    print(f\"Total Repeated Guesses: {total_repeated_guesses}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üèÜ FINAL SCORE:          {final_score:.2f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nBreakdown:\")\n",
    "    print(f\"  + Success bonus:        {(success_rate * 2000):.2f}\")\n",
    "    print(f\"  - Wrong guess penalty:  {total_wrong_guesses * 5}\")\n",
    "    print(f\"  - Repeat guess penalty: {total_repeated_guesses * 2}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "hackathon_results = evaluate_hackathon_hybrid_agent(improved_rl_agent, test_words, num_games=2000)\n",
    "\n",
    "# Save results\n",
    "hackathon_df = pd.DataFrame(hackathon_results['predictions'])\n",
    "hackathon_df.to_csv('hackathon_hybrid_results.csv', index=False)\n",
    "print(\"‚úÖ Results saved to 'hackathon_hybrid_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "53744fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HACKATHON SUBMISSION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìä FINAL RESULTS:\n",
      "  Games Played:       2000\n",
      "  Success Rate:       26.95%\n",
      "  Wrong Guesses:      10807\n",
      "  Repeated Guesses:   0\n",
      "\n",
      "  üèÜ FINAL SCORE:      -135.00\n",
      "\n",
      "üí° OPTIMIZED STRATEGY:\n",
      "  ‚Ä¢ HMM Order: 2 (reliable bigrams)\n",
      "  ‚Ä¢ HMM Smoothing: 0.01 (balanced)\n",
      "  ‚Ä¢ HMM Weighting: 70% context + 20% position + 10% frequency\n",
      "  ‚Ä¢ Learning rate: 0.1 (stable)\n",
      "  ‚Ä¢ Initial epsilon: 0.15 (ultra-low exploration)\n",
      "  ‚Ä¢ Epsilon decay: 0.998\n",
      "  ‚Ä¢ HMM blend weight: 98% (maximum HMM trust)\n",
      "  ‚Ä¢ Discount factor: 0.9\n",
      "  ‚Ä¢ Training episodes: 70,000\n",
      "  ‚Ä¢ Training sample: 60k words\n",
      "  ‚Ä¢ Rewards: +2/letter, -1/wrong, +10/win, -5/loss\n",
      "  ‚Ä¢ KEY: Frequency boost on top 10 letters in first 6 guesses (50% boost)\n",
      "\n",
      "üìà SCORE FORMULA:\n",
      "  Score = (success_rate √ó 2000) - (wrong_guesses √ó 5) - (repeated_guesses √ó 2)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Summary saved to 'hackathon_summary.txt'\n"
     ]
    }
   ],
   "source": [
    "# Final Summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"HACKATHON SUBMISSION SUMMARY\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(\"üìä FINAL RESULTS:\")\n",
    "print(f\"  Games Played:       {hackathon_results['num_games']}\")\n",
    "print(f\"  Success Rate:       {hackathon_results['success_rate']:.2f}%\")\n",
    "print(f\"  Wrong Guesses:      {hackathon_results['total_wrong_guesses']}\")\n",
    "print(f\"  Repeated Guesses:   {hackathon_results['total_repeated_guesses']}\")\n",
    "print(f\"\\n  üèÜ FINAL SCORE:      {hackathon_results['final_score']:.2f}\")\n",
    "\n",
    "print(f\"\\nüí° OPTIMIZED STRATEGY:\")\n",
    "print(f\"  ‚Ä¢ HMM Order: 2 (reliable bigrams)\")\n",
    "print(f\"  ‚Ä¢ HMM Smoothing: 0.01 (balanced)\")\n",
    "print(f\"  ‚Ä¢ HMM Weighting: 70% context + 20% position + 10% frequency\")\n",
    "print(f\"  ‚Ä¢ Learning rate: 0.1 (stable)\")\n",
    "print(f\"  ‚Ä¢ Initial epsilon: 0.15 (ultra-low exploration)\")\n",
    "print(f\"  ‚Ä¢ Epsilon decay: 0.998\")\n",
    "print(f\"  ‚Ä¢ HMM blend weight: 98% (maximum HMM trust)\")\n",
    "print(f\"  ‚Ä¢ Discount factor: 0.9\")\n",
    "print(f\"  ‚Ä¢ Training episodes: 70,000\")\n",
    "print(f\"  ‚Ä¢ Training sample: 60k words\")\n",
    "print(f\"  ‚Ä¢ Rewards: +2/letter, -1/wrong, +10/win, -5/loss\")\n",
    "print(f\"  ‚Ä¢ KEY: Frequency boost on top 10 letters in first 6 guesses (50% boost)\")\n",
    "\n",
    "print(f\"\\nüìà SCORE FORMULA:\")\n",
    "print(f\"  Score = (success_rate √ó 2000) - (wrong_guesses √ó 5) - (repeated_guesses √ó 2)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "# Save summary\n",
    "with open('hackathon_summary.txt', 'w') as f:\n",
    "    f.write(f\"Final Score: {hackathon_results['final_score']:.2f}\\n\")\n",
    "    f.write(f\"Success Rate: {hackathon_results['success_rate']:.2f}%\\n\")\n",
    "    f.write(f\"Games Won: {hackathon_results['wins']}/{hackathon_results['num_games']}\\n\")\n",
    "    f.write(f\"Total Wrong Guesses: {hackathon_results['total_wrong_guesses']}\\n\")\n",
    "    f.write(f\"Total Repeated Guesses: {hackathon_results['total_repeated_guesses']}\\n\")\n",
    "\n",
    "print(\"‚úÖ Summary saved to 'hackathon_summary.txt'\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
